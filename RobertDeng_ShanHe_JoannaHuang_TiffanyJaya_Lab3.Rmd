---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271):
  Lab 3'
author: "Professor Jeffrey Yau"
geometry: margin=1in
output:
  html_document: default
  number_sections: yes
  pdf_document: null
  toc: yes
fontsize: 11pt
---

# Question 1: Forecasting using a SARIMA model

*ECOMPCTNSA.csv*, contains quarterly data of E-Commerce Retail Sales as a Percent of Total Sales. The data can be found at: https://fred.stlouisfed.org/series/ECOMPCTNSA.

# EDA

```{r}
library(car)
library(dplyr)
library(astsa)
library(Hmisc)
library(forecast)
library(ggplot2)
library(fpp)
library(xts)
library(MASS)

#Read in data
e <- read.csv("ECOMPCTNSA.csv")
ts.e <- ts(e$ECOMPCTNSA, frequency = 4, start = c(1999, 4))

#EDA
str(ts.e)
describe(e)
ggplot(e, aes(e$ECOMPCTNSA)) + geom_histogram(bins = 9) + ggtitle("Ecommerce Sales % as of Total Retail Sales") + xlab("Ecommerce Sales %") + ylab("Frequency")
boxplot(ts.e~cycle(ts.e))

#train/test
e.train <- ts.e[time(ts.e) < 2015]
e.test <- ts.e[time(ts.e) > 2014]
str(e.train)
str(e.test)
```
Ecommerce Sales %'s range from 0.7% to 9.5%, skewing right and mostly aggregated in the lower ranges. Mean and variance appear to be stable from the boxplot, but ADF and PP tests can provide more confidence.

```{r}
tsdisplay(e.train, lag = 70)
monthplot(e.train)
monthplot(diff(e.train))
```
Looking at the time series plot, the data shows strong seasonalality and non-stationarity given the upward trend so a seasonal differencing will be used. The monthplot shows the seasonality spike every 4th quarter, likely due to holiday sales. It stands out even more apparently in the difference monthplots. Given there is a small increase in the variance with each level, we will also try taking the logarithms to stabilize the variance.

To better determine whether differencing is necessary, we will use the ADF test.

```{r}
#Stationarity? I think not, difference between the two tests: #https://stats.stackexchange.com/questions/14076/phillips-perron-unit-root-test-instead-of-adf-test
adf.test(e.train)
pp.test(e.train) # when I ran this, I got a p-value of 0.01 meaning we should reject the null of non-stationarity
```
ADF and PP tests show non-stationarity. 

```{r}
#ACF PACF - clear deterministic trend (?)
#Anyway to get the date on x axis working?
#Differencing still shows seasonality
#https://www.otexts.org/fpp/8/5

tsdisplay(diff(e.train, lag=4), lag = 70)
tsdisplay(diff(log(e.train), lag=4), lag = 70)
tsdisplay(diff(diff(e.train, lag=4)), lag = 70)
tsdisplay(diff(diff(log(e.train), lag=4)), lag = 70)

```
There still appears to be a trend when just taking the seasonal difference of e.train and log of e.train. Thus we took a first difference as well. With the first and seasonally differenced log of e.train, the ACF and PACF look like white noise. This suggests that the data is now stationary.

# Modeling
Given our initial analysis, we look at the model behavior of ARIMA models with ranging p and q's, all with first and seasaonl differencing. 
```{r}
for (q in 0:3){
  for(p in 0:3){
      mod <- Arima(log(e.train), order = c(p,1,q),
               seasonal = list(order = c(p,1,q),4),
               method = "ML")

      print(c(p, q, mod$aic, mod$bic))
  }
}
```
The model that returned with the smallest AIC value of-73 and a BIC of -46 was ARIMA(3,1,3)(3,1,3).
```{r}
tm1 <- Arima(log(e.train), order = c(3,1,3),
                         seasonal = list(order = c(3,1,3),4),
                         method = "ML")
summary(tm1)
hist(tm1$residuals)
tsdisplay(tm1$residuals, lag.max = 60)

#Check normal residuals
shapiro.test(tm1$residuals)
qqnorm(tm1$residuals)
qqline(tm1$residuals)
Box.test(tm1$residuals, type = "Ljung-Box")
```
Looking at the residuals of our chosen model, all spoikes are within the significant limits, so our residuals appear to be white noise. Furthermore, the Ljung-Box test shows that residuals have no remaining autocorrelations. The Shapiro-Wilk normality test, histogram and QQ-plot also show that the residuals follow a normal distribution. 


# Forecasting 
We now have a seasonal ARIMA model that passes the required checks and we can move forward with forecasting. We will first see how well our model performs in forecasting the quarterly E-Commerce retail sales in 2015 and 2016.
```{r}
plot(forecast(tm1, h=8), col='red')
par(new=TRUE)
plot(log(ts(e$ECOMPCTNSA, frequency = 4, start = c(1999), end=c(2016))),col="blue")

```
Given the closeness of the red (known values) and blue (predicted values) lines, our model appears to perform very well, and we continue to generate the quarterly forecast for 2017.

```{r}
plot(forecast(tm1, h=12), col='red')
```

```{r}
#auto arima tells me ARIMA(1,1,3) with drift is best but it looks like the residuals are still autocorrelated
tm2 <- auto.arima(log(e.train))
summary(tm2)
hist(tm2$residuals)
tsdisplay(tm2$residuals, lag.max = 60)

#Check normal residuals - not normal
shapiro.test(tm2$residuals)
qqnorm(tm2$residuals)
qqline(tm2$residuals)
Box.test(tm2$residuals, type = "Ljung-Box")
```
# Question 2: Learning how to use the xts library

## Materials covered in Question 2 of this lab

  - Primarily the references listed in this document:

      - "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich. 2008. (xts.pdf)
      - "xts FAQ" by xts Development Team. 2013 (xts_faq.pdf)
      - xts_cheatsheet.pdf
      
https://cran.r-project.org/web/packages/xts/vignettes/xts.pdf

# Task 1:

  1. Read 
    A. the **Introduction** section (Section 1), which only has 1 page of reading of xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    B. the first three questions in"xts FAQ"
        a. What is xts?
        
        Xts is an extensible time series object class that accepts various time series classes to customize the class to the user preference without attributes being lost when coercing classes. This allows the user to manage only a single class of time series without complaint and reduces development time and learning curve.
        
        b. Why should I use xts rather than zoo or another time-series package?
        
        As mentioned above, it easily translates values between different time-series type packages into xts without losing any important attributes in the process and simplifies your workflow.
        
        c. HowdoIinstallxts?
        install.packages("xts")
        
    C. The "A quick introduction to xts and zoo objects" section in this document
        
  2. Read the "A quick introduction to xts and zoo objects" of this document

# A quick introduction to xts and zoo objects

### xts
```xts```
  - stands for eXtensible Time Series
  - is an extended zoo object
  - is essentially matrix + (time-based) index (aka, observation + time)

  - xts is a constructor or a subclass that inherits behavior from parent (zoo); in fact, it extends the popular zoo class. As such. most zoo methods work for xts
  - is a matrix objects; subsets always preserve the matrix form
  - importantly, xts are indexed by a formal time object. Therefore, the data is time-stamped
  - The two most important arguments are ```x``` for the data and ```order.by``` for the index. ```x``` must be a vector or matrix. ```order.by``` is a vector of the same length or number of rows of ```x```; it must be a proper time or date object and be in an increasing order

# Task 2:

  1. Read 
    A. Section 3.1 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
    B. the following questions in "xts FAQ"
        a. How do I create an xts index with millisecond precision?
        
        https://cran.r-project.org/web/packages/xts/vignettes/xts-faq.pdf
```{r}
#allow options for tenth of a second, or millisecond to be shown
options(digits.secs = 3)
#increment by .1 isntead of 1 (seconds)
milli.xts <- xts(1:10, seq(as.POSIXct("1970-01-01"), by=0.1, length=10))
milli.xts
```
        
        b. OK, so now I have my millisecond series but I still canâ€™t see the milliseconds displayed. What went wrong?
        
        See above

  2. Follow the following section of this document


# Creating an xts object and converting to an xts object from an imported dataset

We will create an xts object from a matrix and a time index. First, let's create a matrix and a time index.  The matrix, as it creates, is not associated with the time indext yet.

```{r}
# Create a matrix
x <- matrix(rnorm(200), ncol=2, nrow=100)
colnames(x) <- c("Series01", "Series02")
str(x)
head(x,10)

idx <- seq(as.Date("2015/1/1"), by = "day", length.out = 100)
str(idx)
head(idx)
tail(idx)
```

In a nutshell, xts is a matrix indexed by a time object. To create an xts object, we "bind" the object with the index.  Since we have already created a matrix and a time index (of the same length as the number of rows of the matrix), we are ready to "bind" them together. We will name it *X*.

```{r}
library(xts)
X <- xts(x, order.by=idx)
str(X)
head(X,10)
```
As you can see from the structure of an ```xts``` objevct, it contains both a data component and an index, indexed by an objevct of class ```Date```.

**xtx constructor**
```
xts(x=Null,
    order.by=index(x),
    frequency=NULL,
    unique=NULL,
    tzone=Sys.getenv("TZ"))
```
As mentioned previous, the two most important arguments are ```x``` and ```order.by```.  In fact, we only use these two arguments to create a xts object before.


With a xts object, one can decompose it.

### Deconstructing xts
```coredata()``` is used to extract the data component
```{r}
head(coredata(X),5)
```

```index()``` is used to extract the index (aka times)
```{r}
head(index(X),5)
```
  
### Conversion to xts from other time-series objects

We will use the same dataset "bls_unemployment.csv" that we used in the last live session to illustarte the functions below.


```{r}
# Set working directory
wd <-"~/Documents/Teach/Cal/w271/Labs/Lab3/w271_Lab3_2018Summer/"
setwd(wd)

# Clean up the workspace before we begin
rm(list = ls())

df <- read.csv("bls_unemployment.csv", header=TRUE, stringsAsFactors = FALSE)

# Examine the data structure
  str(df)
  names(df)
  head(df)
  tail(df)

#table(df$Series.id, useNA = "always")
#table(df$Period, useNA = "always")

# Convert a column of the data frame into a time-series object
unemp <- ts(df$Value, start = c(2007,1), end = c(2017,1), frequency = 12)
  str(unemp)
  head(cbind(time(unemp), unemp),5)

# Now, let's convert it to an xts object
df_matrix <- as.matrix(df)
  head(df_matrix)
  str(df_matrix)
  rownames(df)

unemp_idx <- seq(as.Date("2007/1/1"), by = "month", length.out = 
length(df[,1]))
  head(unemp_idx)

unemp_xts <- xts(df$Value, order.by = unemp_idx)
  str(unemp_xts)
  head(unemp_xts)
```

# Task 3:

  1. Read 
    A. Section 3.2 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
  2. Follow the following section of this document
  
# Merging and modifying time series

One of the key strengths of ```xts``` is that it is easy to join data by column and row using a only few different functions. It makes creating time series datasets almost effortless.

The important criterion is that the xts objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.

The major functions is ```merge```.  It works like ```cbind``` or SQL's ```join```:

Let's look at an example. It assumes that you are familiar with concepts of inner join, outer join, left join, and right join.

```{r}
library(quantmod)
#getSymbols("TWTR", src="google"
getSymbols("TWTR")
head(TWTR)
str(TWTR)
```

Note that the date obtained from the getSymbols function of the quantmod library is already an xts object.  As such, we can merge it directly with our unemployment rate xts object constructed above. Nevertheless, it is instructive to examine the data using the View() function to ensure that you understand the number of observations resulting from the joined series.

```{r}
# 1. Inner join
TWTR_unemp01 <- merge(unemp_xts, TWTR, join = "inner")
  str(TWTR_unemp01)
  head(TWTR_unemp01)

# 2. Outer join (filling the missing observations with 99999)
# Basic argument use
TWTR_unemp02 <- merge(unemp_xts, TWTR, join = "outer", fill = 99999)
  str(TWTR_unemp02)
  head(TWTR_unemp02)
  #View(TWTR_unemp02)

# Left join
TWTR_unemp03 <- merge(unemp_xts, TWTR, join = "left", fill = 99999)
  str(TWTR_unemp03)
  head(TWTR_unemp03)
  #View(TWTR_unemp03)
  
# Right join
TWTR_unemp04 <- merge(unemp_xts, TWTR, join = "right", fill = 99999)
  str(TWTR_unemp04)
  head(TWTR_unemp04)
  #View(TWTR_unemp04)
```

# Missing value imputation
xts also offers methods that allows filling missing values using last or previous observation. Note that I include this simply to point out that this is possible. I by no mean certify that this is the preferred method of imputing missing values in a time series.  As I mentioned in live session, the specific method to use in missing value imputation is completely context dependent.

Filling missing values from the last observation
```{r}
# First, let's replace the "99999" values with NA and then exammine the series. 

# Let's examine the first few dozen observations with NA
TWTR_unemp02['2013-10-01/2013-12-15'][,1]

# Replace observations with "99999" with NA and store in a new series
unemp01 <- TWTR_unemp02[, 1]
unemp01['2013-10-01/2013-12-15']
str(unemp01)
head(unemp01)
#TWTR_unemp02[, 1][TWTR_unemp02[, 1] >= 99990] <- NA

unemp02 <- unemp01
unemp02[unemp02 >= 99990] <- NA

cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'])

# Impute the missing values (stored as NA) with the last observation
#TWTR_unemp02_v2a <- na.locf(TWTR_unemp02[,1], 
#                            na.rm = TRUE, fromLast = TRUE) 
unemp03 <- unemp02
unemp03 <- na.locf(unemp03, na.rm = TRUE, fromLast = FALSE) 

# Examine the pre- and post-imputed series
#cbind(TWTR_unemp02['2013-10-01/2013-12-30'][,1], TWTR_unemp02_v2a['2013-10-01/2013-12-15'])
cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'],
unemp03['2013-10-01/2013-12-15'])
```

Another missing value imputation method is linear interpolation, which can also be easily done in xts objects. In the following example, we use linear interpolation to fill in the NA in between months.  The result is stored in ```unemp04```. Note in the following the different ways of imputing missing values.

```{r}
unemp04 <- unemp02
#unemp04['2013-10-01/2014-02-01']
unemp04 <- na.approx(unemp04, maxgap=31)
#unemp04['2013-10-01/2014-02-01']

round(cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'],
unemp03['2013-10-01/2013-12-15'],
unemp04['2013-10-01/2013-12-15']),2)
```

## Calculate difference in time series
A very common operation on time series is to take a difference of the series to transform a non-stationary serier to a stationary series. First order differencing takes the form $x(t) - x(t-k)$ where $k$ denotes the number of time lags. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).

Let's use the ```unemp_xts``` series as examples:
```{r}
str(unemp_xts)
unemp_xts
 
diff(unemp_xts, lag = 1, difference = 1, log = FALSE, na.pad = TRUE)

# calculate the first difference of AirPass using lag and subtraction
#AirPass - lag(AirPass, k = 1)

# calculate the first order 12-month difference if AirPass
diff(unemp_xts, lag = 12, differences = 1)
```

# Task 4:

  1. Read 
    A. Section 3.4 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
    B. the following questions in "xts FAQ"
        a. I am using apply() to run a custom function on my xts series. Why the returned matrix has diâ†µerent dimensions than the original one?

  2. Follow the following two sections of this document

# Apply various functions to time series

The family of ```apply``` functions perhaps is one of the most powerful R function families. In time series, ```xts``` provides ```period.apply```, which takes (1) a time series, (2) an index of endpoints, and (3) a function to apply.  It takes the following general form:
```
period.apply(x, INDEX, FUN, ...)
```

As an example, we use the Twitter stock price series (to be precise, the daily closing price), create an index storing the points corresopnding to the weeks of the daily series, and apply functions to calculate the weekly mean. 

```{r}
# Step 1: Identify the endpoints; in this case, we use weekly time interval. That is, we extract the end index on each week of the series

#View(TWTR)
head(TWTR)
TWTR_ep <- endpoints(TWTR[,4], on = "weeks")
#TWTR_ep

# Step 2: Calculate the weekly mean
TWTR.Close_weeklyMean <- period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = mean)
head(round(TWTR.Close_weeklyMean,2),8)
```

The power of the apply function really comes with the use of custom-defined function. For instance, we can easily 

```{r}
f <- function(x) {
  mean <- mean(x)
  quantile <- quantile(x,c(0.05,0.25,0.50,0.75,0.95))
  sd <- sd(x)
  
  result <- c(mean, sd, quantile)
  return(result)
}
head(round(period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = f),2),10)
```

# Calculate basic rolling statistics  of series by month
Using ```rollapply```, one can calculate rolling statistics of a series:

```{r}
# Calculate rolling mean over a 10-day period and print it with the original series
head(cbind(TWTR[,4], rollapply(TWTR[, 4], 10, FUN = mean, na.rm = TRUE)),15)
```

# Task 5:
1. Read AMAZ.csv and UMCSENT.csv into R as R DataFrames

2. Convert them to xts objects

3. Merge the two set of series together, perserving all of the obserbvations in both set of series
    a. fill all of the missing values of the UMCSENT series with -9999
    
    b. then create a new series, named UMCSENT02, from the original  UMCSENT series replace all of the -9999 with NAs
    
    c. then create a new series, named UMCSENT03, and replace the NAs with the last observation
    
    d. then create a new series, named UMCSENT04, and replace the NAs using linear interpolation.
    
    e. Print out some observations to ensure that your merge as well as the missing value imputation are done correctly. I leave it up to you to decide exactly how many observations to print; do something that makes sense. (Hint: Do not print out the entire dataset!)

4. Calculate the daily return of the Amazon closing price (AMAZ.close), where daily return is defined as $(x(t)-x(t-1))/x(t-1)$. Plot the daily return series.

5. Create a 20-day and a 50-day rolling mean series from the AMAZ.close series.