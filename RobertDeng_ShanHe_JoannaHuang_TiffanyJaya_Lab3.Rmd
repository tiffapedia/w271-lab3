---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271):
  Lab 3'
author: "Professor Jeffrey Yau"
geometry: margin=1in
output:
  html_document: default
  number_sections: yes
  pdf_document: null
  toc: yes
fontsize: 11pt
---

# Question 1: Forecasting using a SARIMA model

*ECOMPCTNSA.csv*, contains quarterly data of E-Commerce Retail Sales as a Percent of Total Sales. The data can be found at: https://fred.stlouisfed.org/series/ECOMPCTNSA.

# EDA

```{r}
library(car)
library(dplyr)
library(astsa)
library(Hmisc)
library(forecast)
library(ggplot2)
library(fpp)
library(xts)
library(MASS)

#Read in data
e <- read.csv("/Users/heshan/Desktop/Berkeley/labs/w271-lab3/data/ECOMPCTNSA.csv")
ts.e <- ts(e$ECOMPCTNSA, frequency = 4, start = c(1999, 4))

#EDA
str(ts.e)
describe(e)
ggplot(e, aes(e$ECOMPCTNSA)) + geom_histogram(bins = 9) + ggtitle("Ecommerce Sales % as of Total Retail Sales") + xlab("Ecommerce Sales %") + ylab("Frequency")
boxplot(ts.e~cycle(ts.e))

#train/test
e.train <- ts.e[time(ts.e) < 2015]
e.test <- ts.e[time(ts.e) > 2014]
str(e.train)
str(e.test)
```
Ecommerce Sales %'s range from 0.7% to 9.5%, skewing right and mostly aggregated in the lower ranges. Mean and variance appear to be stable from the boxplot, but ADF and PP tests can provide more confidence.

```{r}
tsdisplay(e.train, lag = 70)
monthplot(e.train)
monthplot(diff(e.train))
```

Looking at the time series plot, the data shows strong seasonalality and non-stationarity given the upward trend, with increasing variance, both a seasonal differencing and first-order difference may be needed The monthplot shows the seasonality spike every 4th quarter, likely due to holiday sales. It stands out even more apparently in the difference monthplots. Given there is a small increase in the variance with each level, we will also try taking the logarithms to stabilize the variance.

To better determine whether differencing is necessary, we will use the ADF test.

```{r}
#Stationarity? I think not, difference between the two tests: #https://stats.stackexchange.com/questions/14076/phillips-perron-unit-root-test-instead-of-adf-test
adf.test(e.train)
pp.test(e.train) # when I ran this, I got a p-value of 0.01 meaning we should reject the null of non-stationarity
```
ADF and PP tests show non-stationarity. 

```{r}
#ACF PACF - clear deterministic trend (?)
#Anyway to get the date on x axis working?
#Differencing still shows seasonality
#https://www.otexts.org/fpp/8/5

tsdisplay(diff(e.train, lag=4), lag = 70)
tsdisplay(diff(log(e.train), lag=4), lag = 70)
tsdisplay(diff(diff(e.train, lag=4)), lag = 70)
tsdisplay(diff(diff(log(e.train), lag=4)), lag = 70)

```
There still appears to be a trend when just taking the seasonal difference of e.train and log of e.train. Thus we took a first difference as well. With the first and seasonally differenced log of e.train, the ACF and PACF look better but we still see a significant PACF at lag 4. This may suggest a seasonal first order AR is needed for our model. 

# Modeling
Given our initial analysis, we look at the model behavior of ARIMA models with ranging p and q's, all with first and seasaonl differencing. 
```{r}
for (q in 0:3){
  for(p in 0:3){
      mod <- Arima(log(e.train), order = c(p,1,q),
               seasonal = list(order = c(p,1,q),4),
               method = "ML")

      print(c(p, q, mod$aic, mod$bic))
  }
}

```
The model that returned with the smallest AIC value of-73 and a BIC of -46 was ARIMA(3,1,3)(3,1,3).
```{r}
tm1 <- Arima(log(e.train), order = c(3,1,3),
                         seasonal = list(order = c(3,1,3),4),
                         method = "ML")
summary(tm1)
hist(tm1$residuals)
tsdisplay(tm1$residuals, lag.max = 60)

#Check normal residuals
shapiro.test(tm1$residuals)
qqnorm(tm1$residuals)
qqline(tm1$residuals)
Box.test(tm1$residuals, type = "Ljung-Box")
```
Looking at the residuals of our chosen model, all spoikes are within the significant limits, so our residuals appear to be white noise. Furthermore, the Ljung-Box test shows that residuals have no remaining autocorrelations. The Shapiro-Wilk normality test, histogram and QQ-plot also show that the residuals follow a normal distribution. 

# Forecasting 
We now have a seasonal ARIMA model that passes the required checks and we can move forward with forecasting. We will first see how well our model performs in forecasting the quarterly E-Commerce retail sales in 2015 and 2016.
```{r}
plot(forecast(tm1, h=8), col='red')
par(new=TRUE)
plot(log(ts(e$ECOMPCTNSA, frequency = 4, start = c(1999), end=c(2016))),col="blue")

```
Given the closeness of the red (known values) and blue (predicted values) lines, our model appears to perform very well, and we continue to generate the quarterly forecast for 2017.

```{r}
plot(forecast(tm1, h=12), col='red')
```

```{r}
#auto arima tells me ARIMA(1,1,3) with drift is best but it looks like the residuals are still autocorrelated
tm2 <- auto.arima(log(e.train))
summary(tm2)
hist(tm2$residuals)
tsdisplay(tm2$residuals, lag.max = 60)

#Check normal residuals - not normal
shapiro.test(tm2$residuals)
qqnorm(tm2$residuals)
qqline(tm2$residuals)
Box.test(tm2$residuals, type = "Ljung-Box")
```
# Question 2: Learning how to use the xts library











# Apply various functions to time series

The family of ```apply``` functions perhaps is one of the most powerful R function families. In time series, ```xts``` provides ```period.apply```, which takes (1) a time series, (2) an index of endpoints, and (3) a function to apply.  It takes the following general form:
```
period.apply(x, INDEX, FUN, ...)
```

As an example, we use the Twitter stock price series (to be precise, the daily closing price), create an index storing the points corresopnding to the weeks of the daily series, and apply functions to calculate the weekly mean. 

```{r}
# Step 1: Identify the endpoints; in this case, we use weekly time interval. That is, we extract the end index on each week of the series

#View(TWTR)
head(TWTR)
TWTR_ep <- endpoints(TWTR[,4], on = "weeks")
#TWTR_ep

# Step 2: Calculate the weekly mean
TWTR.Close_weeklyMean <- period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = mean)
head(round(TWTR.Close_weeklyMean,2),8)
```

The power of the apply function really comes with the use of custom-defined function. For instance, we can easily 

```{r}
f <- function(x) {
  mean <- mean(x)
  quantile <- quantile(x,c(0.05,0.25,0.50,0.75,0.95))
  sd <- sd(x)
  
  result <- c(mean, sd, quantile)
  return(result)
}
head(round(period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = f),2),10)
```

# Calculate basic rolling statistics  of series by month
Using ```rollapply```, one can calculate rolling statistics of a series:

```{r}
# Calculate rolling mean over a 10-day period and print it with the original series
head(cbind(TWTR[,4], rollapply(TWTR[, 4], 10, FUN = mean, na.rm = TRUE)),15)
```

# Task 5:
1. Read AMAZ.csv and UMCSENT.csv into R as R DataFrames

```{r}
a <- read.csv("AMAZ.csv")
u <- read.csv("UMCSENT.csv")
#describe(a)
#describe(u)
```

2. Convert them to xts objects
```{r}
ats <- xts(a[,-1], order.by=as.POSIXct(a[,1]))
uts <- xts(u[,-1], order.by=as.POSIXct(u[,1]))
```

3. Merge the two set of series together, perserving all of the obserbvations in both set of series
    a. fill all of the missing values of the UMCSENT series with -9999
```{r}
UMCSENT <- merge(ats, uts, join = "outer", fill = -9999)
#head(UMCSENT)
#tail(UMCSENT)
#describe(UMCSENT)
```
    b. then create a new series, named UMCSENT02, from the original  UMCSENT series replace all of the -9999 with NAs
```{r}
UMCSENT02 <- UMCSENT
UMCSENT02[UMCSENT02 == -9999] <- NA
head(UMCSENT02)
```
    c. then create a new series, named UMCSENT03, and replace the NAs with the last observation
```{r}
UMCSENT03 <- UMCSENT02
UMCSENT03 <- na.locf(UMCSENT03, na.rm = TRUE, fromLast = TRUE) 
#head(UMCSENT03)
#describe(UMCSENT03)
UMCSENT03['2007-01-03']
UMCSENT02['2007-01-03']
```

    d. then create a new series, named UMCSENT04, and replace the NAs using linear interpolation.

```{r}
UMCSENT04 <- UMCSENT02
UMCSENT04 <-na.approx(UMCSENT04, maxgap=31)
head(UMCSENT04)

#Check if values for uts were replaced
UMCSENT04['2007-01-03']
UMCSENT02['2007-01-03']

```

    e. Print out some observations to ensure that your merge as well as the missing value imputation are done correctly. I leave it up to you to decide exactly how many observations to print; do something that makes sense. (Hint: Do not print out the entire dataset!)


4. Calculate the daily return of the Amazon closing price (AMAZ.close), where daily return is defined as $(x(t)-x(t-1))/x(t-1)$. Plot the daily return series.

```{r}
daily_return <- (ats[,4] - lag(ats[,4], k = 1))/ (lag(ats[,4], k = 1))

plot(daily_return)

```

5. Create a 20-day and a 50-day rolling mean series from the AMAZ.close series.

```{r}
head(cbind(ats[,4], rollapply(ats[, 4], 20, FUN = mean, na.rm = TRUE)),30)
head(cbind(ats[,4], rollapply(ats[, 4], 50, FUN = mean, na.rm = TRUE)),60)
```

