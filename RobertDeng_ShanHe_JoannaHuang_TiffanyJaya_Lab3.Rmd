---
title: "W271 Lab 3"
author: "Tiffany Jaya, Joanna Huang, Robert Deng, Shan He"
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

```{r message=FALSE}
# add packages
library(forecast)
library(knitr)
library(stats)
library(tseries)
library(xts)
# prevent source code from running off the page
opts_chunk$set(tidy.opts=list(width.cutoff=70), tidy=TRUE)
# remove all objects from current workspace
rm(list = ls())
# set seed number to reproduce results
set.seed(1)
# load data
raw.sales <- read.csv('./data/ECOMPCTNSA.csv', header=TRUE, sep=',')
```

## Question 1: Forecasting using a SARIMA model

Since the emergence of the internet, more and more people are shopping at online retailers than brick-and-mortar stores. E-commerce is on the rise, and we would like to see what percentage of total retail sales e-commerce is accounted for in the fourth quarter of 2017. With data from the US Census Bureau ranging from 1999 to 2016, we were able to estimate it to be 10.20% using the seasonal autoregresive integrated moving average model (or SARIMA for short). While the number does not seem substantial compare to the perceived value of e-commerce, we have to remember that retail sales include motor vehicles, gas stations, and grocery stores where e-commerce has yet to play a major role in the field. 

The SARIMA model that we use for the projected forecast is $\text{ARIMA}(0,1,0)(0,1,1)_4$. 

**Exploration Data Analysis**

The first step once we obtained the dataset was to examine its structure. 

```{r}
# convert raw data into a time-series object
sales <- ts(raw.sales$ECOMPCTNSA, start=c(1999,4), frequency=4)
# hold out test data for verification in forecast
sales.train <- ts(sales[time(sales) < 2015], start=c(1999,4), frequency=4) # 1999-2014
sales.test <- ts(sales[time(sales) >= 2015], start=c(2015,1), frequency=4) # 2015-2016
# examine the structure
kable(summary(raw.sales)) 
head(sales); tail(sales)
```

We were able to determine that there was no missing value among the 69 observations with sales appearing to increase  overtime from 0.7% in the 4th quarter of 1999 to 9.5% in the 4th quarter of 2016. To confirm, we plot the time series as well as its associating MA(4) model. If the data expressed seasonality every quarter, the MA(4) model smooths out the variances and acts as an annual trend with the seasonal effects within each quarter removed.

```{r fig.show='hold', fig.align='center', out.width='49%'}
# using the entire dataset
plot(sales, ylab = 'e-commerce sales (%)', main='Quarterly Series (entire dataset)')
lines(ma(sales, order=4, centre=T), col='blue')
acf(sales)
# using the train dataset
plot(sales.train, ylab = 'e-commerce sales (%)', main='Quarterly Series (train dataset)')
lines(ma(sales.train, order=4, centre=T), col='blue')
acf(sales.train)
# remove trend to see seasonality
plot(sales - ma(sales, order=4, centre=T), main='Seasonality (Proof Not Additive Model)') 
plot(sales / ma(sales, order=4, centre=T), main='Seasonality (Proof Is Multiplicative Model)')
```

Given the upward trend and increasing variance, the series is a multiplicative model that is non-stationary with quarterly seasonality. The autocorrelation function further substantiates the series's non-stationary because of its slow decay and $r_1$s that are large and positive ($r_1$ indicates how successive values of y relate to each other). For this reason, we will perform two operations. One, we will difference the series to stabilize the mean. And two, we will apply a Box-Cox transformation (logarithm and power transformation) to stabilize the variance. We verify if differencing was necessary by running the unit root test. 

```{r warning=FALSE}
# unit root test
cbind(adf.test(sales.train, alternative='stationary')$p.value, 
      kpss.test(sales.train)$p.value, 
      ndiffs(sales.train),
      nsdiffs(sales.train))
```

Large p-value in the Augmented Dickey-Fuller (ADF) test and small p-value in the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test confirm our intuition to difference the time series. As suggested by the non-seasonal (ndiffs) and seasonal (nsdiff) unit test, we will perform a non-seasonal difference to the data once and a seasonal difference once in order to make the series stationary in mean. The ADF and KPSS tests we run afterwards validate our differencing decision. We apply log and power transformation to the difference using the Box-Cox transformation by first finding the best lambda value that will give the optimal uniformity in the seasonal variation before administering the said transformation. With a lambda value of 0.01467236, the transformation is similar to a log transformation.

```{r fig.show='hold', fig.align='center', out.width='49%', warning=FALSE}
# find the best lambda for Box-Cox transformation
# lambda = 0.01467236, similar to a log-transformation
lambda <- BoxCox.lambda(sales.train) 
# first-order non-seasonal differenced 
plot(diff(sales.train),
     main='1st order non-seasonal difference')
# ^ with Box-Cox transformed
plot(diff(BoxCox(sales.train, lambda)), 
     main='1st order non-seasonal difference w/ Box-Cox transform')
# ^ with first-order seasonal differenced
plot(diff(diff(BoxCox(sales.train, lambda), lag=4)), 
     main='1st order non-seasonal and seasonal difference w/ Box-Cox transform')
# ^ and ACF and PACF
tsdisplay(diff(diff(BoxCox(sales.train, lambda), lag=4)))
# unit root test on first-order differenced log-transformed series
cbind(adf.test(diff(diff(BoxCox(sales.train, lambda), lag=4)), alternative='stationary')$p.value, 
      kpss.test(diff(diff(BoxCox(sales.train, lambda), lag=4)))$p.value)
```

**Modeling** 

Looking at the autocorrelation function (ACF) and partial autocorrelation function (PACF), we estimate the best-fitting model to be $\text{ARIMA}(0,1,0)(0,1,1)_4$. Our reasoning is as follows:

* Since we perform first order non-seasonal and seasonal difference, the non-seasonal difference d and seasonal difference D are equal to 1. 
* With no significant spike in the non-seasonal lags of ACF and PACF plots, it suggests a possible non-seasonal AR(0) and MA(0) term.
* Since the seasonal correlograms in the ACF plot do not tail off to zero but those in the PACF plot does after lag 4, it signifies a potential moving average model. The only significant spike in ACF is at lag 4; all other autocorrelations are not significant. For this reason, it suggests a potential seasonal MA(1) term. 

```{r}
(base.m <- Arima(BoxCox(sales.train, lambda), order=c(0,1,0),seasonal=list(order=c(0,1,1),4)))
```

By iterating through multiple parameters, we can confirm whether or not this model is the best-fitting model under the AICc criterion. We chose AICc instead of AIC since the number of observations, 69, is small and AICc can address AIC's potential problem of overfitting for small sample sizes.

```{r}
best.manual.m <- base.m
for(p in 0:2) for(q in 0:2) for(d in 1:2) for(P in 0:2) for(Q in 0:2) for(D in 1:2) {
  m <- Arima(BoxCox(sales.train, lambda), order=c(p,d,q), seasonal=list(order=c(P,D,Q)))
  if(m$aicc < best.manual.m$aicc) best.manual.m <- m
}
best.manual.m
```

What we have found is that $\text{ARIMA}(0,1,0)(0,1,2)_4$ has a lower AICc score than our estimated model we derived earlier $\text{ARIMA}(0,1,0)(0,1,1)_4$. In other words, $\text{ARIMA}(0,1,0)(0,1,2)_4$ may better explain the data but that fit might not be worth it at the cost of a loss in parsimony since we have to impose an additional seasonal MA lag into our estimated model. Similarly, $\text{ARIMA}(0,1,0)(0,1,1)_4$ may be more parsimonious, but it might not explain the data as well as $\text{ARIMA}(0,1,0)(0,1,2)_4$.

We compare our generated best-fitting model $\text{ARIMA}(0,1,0)(0,1,2)_4$ to the one generated by the auto.arima function and found it to be the same.

```{r}
(best.auto.m <- auto.arima(BoxCox(sales.train, lambda), ic='aicc', stepwise=FALSE, approximation=FALSE))
```

For this reason, the two models we will compare moving forward are $\text{ARIMA}(0,1,0)(0,1,1)_4$, which is more parsimonous, and $\text{ARIMA}(0,1,0)(0,1,2)_4$, which has a better fit.

```{r}
m1 <- base.m
m2 <- best.auto.m
```

**Validating the model**

Before we can forecast what percentage of total retail sales e-commerce sales will be in the future, we first need to validate that the residuals from the two models result in the following properties:

* uncorrelated, meaning there is no information left in the residuals that can be used in computing the forecast
* zero mean
* constant variance
* normally distributed

```{r fig.show='hold', fig.align='center', out.width='49%'}
checkresiduals(m1$residuals)
h <- min(2 * 4, nrow(sales.train)/5) # min(2m, T/5) 
Box.test(m1$residuals, type='Ljung-Box', lag=h) 
shapiro.test(m1$residuals)
```

```{r fig.show='hold', fig.align='center', out.width='49%'}
checkresiduals(m2$residuals)
h <- min(2 * 4, nrow(sales.train)/5) # min(2m, T/5) 
Box.test(m2$residuals, type='Ljung-Box', lag=h) 
shapiro.test(m2$residuals)
```

Looking at the ACF plots, all spikes of the two models are within the significant limits, meaning that the residuals are uncorrelated to one another. We then perform a test on a group of autocorrelations with the Box-Ljung test. With large p-values, the Box-Ljung test validates our assumption that the residuals are uncorrelated. The time plot of the residuals shows that the variation of the residuals stays approximately the same for both models, so we can treat the residual variance as constant. However, even with a mean close to zero, the histogram of all models suggests that it follows more of a negative skewed distribution than normal. The Shapiro-Wilk test confirms the non-normality of the distribution for the two models. What this signifies is that when we perform a prediction in the following section, its forecast will generally be quite good but prediction intervals computed assuming a normal distribution may be inaccurate.

**Forecasting**

Now that we have validated our models, it is time to extrapolate what percentage of e-sales commerce constitutes the total retail sales. First, we compare the forecasts of all the models to the hold out test data from 2015 till 2016 to see if their predictions are comparable to the actual. Then we plot out the forecasts. The blue represents the forecast and the orange represents the test data.

```{r}
sales.test
InvBoxCox(predict(m1, 3*4)$pred, lambda)
InvBoxCox(predict(m2, 3*4)$pred, lambda)
```

```{r fig.show='hold', fig.align='center', out.width='32%'}
ts.plot(cbind(sales.train, sales.test, InvBoxCox(predict(m1, 4*3)$pred, lambda)), 
        col=c('black', 'orange', 'blue'), lty=c(1,1,2), 
        main='Forecasts to 2017 with manual ARIMA(0,1,0)(0,1,1)[4]')
ts.plot(cbind(sales.train, sales.test, InvBoxCox(predict(m2, 4*3)$pred, lambda)), 
        col=c('black', 'orange', 'blue'), lty=c(1,1,2), 
        main='Forecasts to 2017 with manual ARIMA(0,1,0)(0,1,2)[4]')
plot(forecast(m1))
plot(forecast(m2))
```

Both the forecasts as well as the graphs tell us that our parsimonous model $\text{ARIMA}(0,2,1)(0,2,1)_4$ predict much more closely to the test data than the autogenerated one $\text{ARIMA}(0,2,1)(0,2,2)_4$. We use this model to determine that e-commerce makes up approximately 10.20% of all retail sales by the fourth quarter of 2017.

## Question 2: Learning how to use the xts library

If we could select one company to represent the e-commerce trend, Amazon is likely to be the first company that comes to mind.


1. Read AMAZ.csv and UMCSENT.csv into R as R DataFrames.

```{r}
raw.amaz <- read.csv('./data/AMAZ.csv', header=TRUE, sep=',')
raw.sent <- read.csv('./data/UMCSENT.csv', header=TRUE, sep=',')
```

```{r}
raw.sent
```

2. Convert them to xts objects.

```{r}
# set local timezone
Sys.setenv(TZ='America/Los_Angeles')
# assume stock data is collected in EST
amaz <- xts(raw.amaz[,-1], order.by=as.POSIXct(raw.amaz[,1], tz='EST'))
# assume sentiment data is collected in EST
sent <- xts(raw.sent[,-1], order.by=as.POSIXct(raw.sent[,1], tz='EST'))
```

```{r}
ats <- amaz
uts <- sent
```


3. Merge the two set of series together, preserving all of the observations in both set of series.
    a. Fill all of the missing values of the UMCSENT series with -9999.

```{r}
UMCSENT <- merge(ats, uts, join = "outer", fill = -9999)
#head(UMCSENT)
#tail(UMCSENT)
#describe(UMCSENT)
```

    b. Then create a new series, named UMCSENT02, from the original UMCSENT series and replace all of the -9999 with NAs.

```{r}
UMCSENT02 <- UMCSENT
UMCSENT02[UMCSENT02 == -9999] <- NA
head(UMCSENT02)
```

    c. Then create a new series, named UMCSENT03, and replace the NAs with the last observation.

```{r}
UMCSENT03 <- UMCSENT02
UMCSENT03 <- na.locf(UMCSENT03, na.rm = TRUE, fromLast = TRUE) 
#head(UMCSENT03)
#describe(UMCSENT03)
UMCSENT03['2007-01-03']
UMCSENT02['2007-01-03']
```

    d. Then create a new series, named UMCSENT04, and replace the NAs using linear interpolation.

```{r}
UMCSENT04 <- UMCSENT02
UMCSENT04 <-na.approx(UMCSENT04, maxgap=31)
head(UMCSENT04)

#Check if values for uts were replaced
UMCSENT04['2007-01-03']
UMCSENT02['2007-01-03']

```

    e. Print out some observations to ensure that your merge as well as the missing value imputation are done correctly. I leave it up to you to decide exactly how many observations to print; do something that makes sense. (Hint: Do not print out the entire dataset!)

```{r}
#TODO!!!!!!! 
#check Jan 3 for both raw datasets
ats['2007-01-03']
uts['2007-01-03']

#not in uts, check two closesest uts values
uts['2007-01-01']
uts['2007-02-01']

#check merge
UMCSENT04['2007-01-03']

#check interpolation
coredata(UMCSENT04['2007-01-03', 6]) == coredata(uts['2007-01-01']) - (coredata(uts['2007-01-01']) - coredata(uts['2007-02-01'])) * 2/31 

```

4. Calculate the daily return of the Amazon closing price (AMAZ.close), where daily return is defined as $(x(t)-x(t-1))/x(t-1)$. Plot the daily return series.

```{r fig.show='hold', fig.align='center', out.width='49%'}
daily_return <- (ats[,4] - lag(ats[,4], k = 1))/ (lag(ats[,4], k = 1))

plot(daily_return)

```
Looking at Amazon's closing price between January 2017 to January 2013, we see much volatility. A few trends that may be gauged is that January to July generally sees lower daily returns and peaks of 1+ returns happens post-July (which may be related to product launch timelines). These trends hold with the exception of January-July 2009 which had unprecedented returns that may be due to the acquisition of Zappos during that time. 

5. Create a 20-day and a 50-day rolling mean series from the AMAZ.close series.

```{r}
head(cbind(ats[,4], rollapply(ats[, 4], 20, FUN = mean, na.rm = TRUE)),30)
head(cbind(ats[,4], rollapply(ats[, 4], 50, FUN = mean, na.rm = TRUE)),60)
```


# 1. ECOMPCTNSA: E-commerce retail sales as a percent of total sales
# https://fred.stlouisfed.org/series/ECOMPCTNSA
