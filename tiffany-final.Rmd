---
title: "W271 Lab 3"
author: "Tiffany Jaya, Joanna Huang, Shan He, Robert Deng"
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

```{r}
# add packages
library(forecast)
library(knitr)
library(stats)
library(xts)
# prevent source code from running off the page
opts_chunk$set(tidy.opts=list(width.cutoff=70), tidy=TRUE)
# remove all objects from current workspace
rm(list = ls())
# set seed number to reproduce results
set.seed(1)
# load data
# 1. ECOMPCTNSA: E-commerce retail sales as a percent of total sales
# https://fred.stlouisfed.org/series/ECOMPCTNSA
raw.sales <- read.csv('./data/ECOMPCTNSA.csv', header=TRUE, sep=',')
```

# Question 1: Forecasting using a SARIMA model

TODO: change the % and ARIMA model

Since the emergence of the internet, more and more people are shopping at online retailers than brick-and-mortar stores. E-commerce is on the rise, and we would like to see what percentage of total retail sales e-commerce is accounted for in the fourth quarter of 2017. With data from the US Census Bureau ranging from 1999 to 2016, we were able to estimate using the seasonal autoregresive integrated moving average model (or SARIMA for short) that e-commerce makes up approximately 10.44% of all retail sales. While the number does not seem substantial compare to the perceived value of e-commerce, we have to remember that retail sales include motor vehicles, gas stations, and grocery stores where e-commerce has yet to play a major role in the field. 

The SARIMA model that we use for the projected forecast is $\text{ARIMA}(0,2,1)(2,2,1)_4$. 

## Exploration Data Analysis 

The first step once we obtained the dataset was to examine its structure. 

```{r}
# convert raw data into a time-series object
sales <- ts(raw.sales$ECOMPCTNSA, start=c(1999,4), frequency=4)
sales.train <- ts(sales[time(sales) < 2015], start=c(1999,4), frequency=4) # 1999-2014
sales.test <- ts(sales[time(sales) >= 2015], start=c(2015,1), frequency=4) # 2015-2016
# examine the structure
kable(summary(raw.sales)) 
head(sales); tail(sales)
```

We were able to determine that there was no missing value among the 69 observations with sales appearing to increase  overtime from 0.7% in the 4th quarter of 1999 to 9.5% in the 4th quarter of 2016. To confirm, we plot the time series as well as its associating MA(4) model. If the data expressed seasonality every quarter, the MA(4) model smooths out the variances and acts as an annual trend with the seasonal effects within each quarter removed.

```{r fig.show='hold', fig.align='center', out.width='49%'}
# using the entire dataset
plot(sales, ylab = 'e-commerce sales (%)', main='E-commerce Quarterly Series (entire dataset)')
lines(ma(sales, order=4, centre=T), col='blue')
acf(sales)
# using the train dataset
plot(sales.train, ylab = 'e-commerce sales (%)', main='E-commerce Quarterly Series (train dataset)')
lines(ma(sales.train, order=4, centre=T), col='blue')
acf(sales.train)
```

Given the upward trend and increasing variance, the series is non-stationary with quarterly seasonality. The autocorrelation function further substantiates the series's non-stationary because of its slow decay and $r_1$s that are large and positive ($r_1$ indicates how successive values of y relate to each other). For this reason, we will perform two operations. One, we will difference the series to stabilize the mean. And two, we will apply a Box-Cox transformation (logarithm and power transformation) to stabilize the variance. We verify if differencing was necessary by running the unit root test. 

```{r}
# unit root test
# H0: non-stationary, HA: p < 0.05, stationary
adf.test(sales.train, alternative='stationary')
# H0: stationary, HA: p < 0.05, non-stationary
kpss.test(sales.train)
# number of differences required
nsdiffs(sales.train)
```

Large p-value in the Augmented Dickey-Fuller (ADF) test and small p-value in the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test confirm our intuition to difference the time series. Although the seasonal unit test suggests to difference the data once, we decide to difference it a second time in order to make the series stationary (as implied by the two unit root tests we run before: ADF and KPSS). We apply log and power transformation to the difference using the Box-Cox transformation by first finding the best lambda value that will give the optimal uniformity in the seasonal variation before administering the said transformation.

```{r fig.show='hold', fig.align='center', out.width='49%'}
# find the best lambda for Box-Cox transformation
lambda <- BoxCox.lambda(sales.train) # lambda = 0.01467236
# first-order differenced series plot
plot(diff(sales.train, difference=1))
# first-order differenced BoxCox-transformed series plot
plot(diff(BoxCox(sales.train, lambda), difference=1))
# first-order differenced BoxCox-transformed series
tsdisplay(diff(BoxCox(sales.train, lambda), lag=4, difference=1))
# second-order differenced BoxCox-transformed series
tsdisplay(diff(BoxCox(sales.train, lambda), lag=4, difference=2))
# unit root test on first-order differenced log-transformed series
adf.test(diff(BoxCox(sales.train, lambda), lag=4, difference=1), alternative='stationary')
kpss.test(diff(BoxCox(sales.train, lambda), lag=4, difference=1))
# unit root test on first-order differenced log-transformed series
adf.test(diff(BoxCox(sales.train, lambda), lag=4, difference=2), alternative='stationary')
kpss.test(diff(BoxCox(sales.train, lambda), lag=4, difference=2))
```

## Modeling 

Looking at the autocorrelation function (ACF) and partial autocorrelation function (PACF) of the second order differenced, we estimate the best-fitting model to be $\text{ARIMA}(1,2,1)(1,2,1)$. 

* In the non-seasonal lags, the significant spike at lag 1 in the PACF suggests a possible non-seasonal AR(1) term. 
* In the seasonal lags, the significant spike at lag 4 in the PACF suggests a possible seasonal AR(1) term.
* In the non-seasonal lags, the significant spike at lag 1 in the ACF suggests a possible non-seasonal MA(1) term.
* In the seasonal lags, the significant spike at lag 4 in the ACF suggests a possible seasonal MA(1) term.

```{r}
(base.m <- Arima(BoxCox(sales.train, lambda), order=c(1,2,1),seasonal=list(order=c(1,2,1),4)))
```

By iterating through multiple parameters, we can confirm that this model is the best-fitting model under the AICc criterion.

```{r}
best.manual.m <- base.m
for(P in 0:2) for(Q in 0:2) for(p in 0:2) for(q in 0:2) {
  m <- Arima(BoxCox(sales.train, lambda), order=c(P,2,Q), seasonal=list(order=c(p,2,q)))
  if(m$aicc < best.m$aicc) best.manual.m <- m
}
best.manual.m
```

We compare our generated best-fitting model to the one generated by the auto.arima function and found it to be a first-order differenced series: $\text{ARIMA}(0,1,0)(0,1,2)$.

```{r}
(best.auto.m <- auto.arima(BoxCox(sales.train, lambda), ic='aicc', stepwise=FALSE, approximation=FALSE))
```

## Validating 

uncorrelated (meaning there is no information left in the residuals that can be used in computing forecasts), zero mean, constant variance, normaly distributed 

A good forecasting method will yield residuals with the following properties:

The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

Adjusting for bias is easy: if the residuals have mean mm, then simply add mm to all forecasts and the bias problem is solved. Fixing the correlation problem is harder and we will not address it until Chapter 8.

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

The residuals have constant variance.
The residuals are normally distributed.
```{r fig.show='hold', fig.align='center', out.width='49%'}
# verify normality of the residuals
tsdisplay(best.m$residuals)
hist(best.m$residuals, main='Histogram of residuals')
acf(best.m$residuals, main='ACF of residuals')
Box.test(best.m$residuals, type='Ljung-Box') 

# shapiro.test(best.m$residuals)
# qqnorm(best.m$residuals); qqline(best.m$residuals)
```
The mean of the residuals is very close to zero and there is a significant correlation in the residuals series. The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data, so the residual variance can be treated as constant. However, the histogram suggests that the residuals may not follow a normal distribution --- the left tail looks a little too long. Consequently, forecasts from this method will probably be quite good but prediction intervals computed assuming a normal distribution may be inaccurate.

Looking at the residuals of our chosen model, all spoikes are within the significant limits, so our residuals appear to be white noise. Furthermore, the Ljung-Box test shows that residuals are independent (null hypothesis = independence). The Shapiro-Wilk normality test, histogram and QQ-plot also show that the residuals follow a normal distribution. 

## Forecasting

Use both in-sample and out-of-sample model performance. 

When training your model, exclude the series from 2015 and 2016. For the out-of-sample forecast, measure your modelâ€™s performance in forecasting the quarterly E-Commerce retail sales in 2015 and 2016. Discuss the model performance. Also forecast beyond the observed time-period of the series. Specifically, generate quarterly forecast for 2017.
```{r}
# train the best model excluding series from 2015-2016
best.m.train <- Arima(log(sales.train), order=c(0,1,0), seasonal=list(order=c(0,1,2))) 
ts.plot(cbind(window(sales, start=1999, end=2015), exp(predict(best.m.train, 4*3)$pred)), xaxt='n', lty=1:2, axes=FALSE)
axis(1, at=seq(from=1999, to=2017), las=2, axes=FALSE)
```



# Question 2: Learning how to use the xts library

If we could select one company to represent the e-commerce trend, Amazon is likely to be the first company that comes to mind.


1. Read AMAZ.csv and UMCSENT.csv into R as R DataFrames.

```{r}
raw.amaz <- read.csv('./data/AMAZ.csv', header=TRUE, sep=',')
raw.sent <- read.csv('./data/UMCSENT.csv', header=TRUE, sep=',')
amaz <- ts()
```

```{r}
raw.sent
```

2. Convert them to xts objects.

```{r}
# set local timezone
Sys.setenv(TZ='America/Los_Angeles')
# assume stock data is collected in EST
amaz <- xts(raw.amaz[,-1], order.by=as.POSIXct(raw.amaz[,1], tz='EST'))
# assume sentiment data is collected in EST
sent <- xts(raw.sent[,-1], order.by=as.POSIXct(raw.sent[,1], tz='EST'))
```

3. Merge the two set of series together, preserving all of the observations in both set of series.
    a. Fill all of the missing values of the UMCSENT series with -9999.

```{r}
UMCSENT <- merge(ats, uts, join = "outer", fill = -9999)
#head(UMCSENT)
#tail(UMCSENT)
#describe(UMCSENT)
```

    b. Then create a new series, named UMCSENT02, from the original UMCSENT series and replace all of the -9999 with NAs.

```{r}
UMCSENT02 <- UMCSENT
UMCSENT02[UMCSENT02 == -9999] <- NA
head(UMCSENT02)
```

    c. Then create a new series, named UMCSENT03, and replace the NAs with the last observation.

```{r}
UMCSENT03 <- UMCSENT02
UMCSENT03 <- na.locf(UMCSENT03, na.rm = TRUE, fromLast = TRUE) 
#head(UMCSENT03)
#describe(UMCSENT03)
UMCSENT03['2007-01-03']
UMCSENT02['2007-01-03']
```

    d. Then create a new series, named UMCSENT04, and replace the NAs using linear interpolation.

```{r}
UMCSENT04 <- UMCSENT02
UMCSENT04 <-na.approx(UMCSENT04, maxgap=31)
head(UMCSENT04)

#Check if values for uts were replaced
UMCSENT04['2007-01-03']
UMCSENT02['2007-01-03']

```

    e. Print out some observations to ensure that your merge as well as the missing value imputation are done correctly. I leave it up to you to decide exactly how many observations to print; do something that makes sense. (Hint: Do not print out the entire dataset!)

```{r}
TODO!!!!!!! 
```

4. Calculate the daily return of the Amazon closing price (AMAZ.close), where daily return is defined as $(x(t)-x(t-1))/x(t-1)$. Plot the daily return series.

```{r fig.show='hold', fig.align='center', out.width='49%'}
daily_return <- (ats[,4] - lag(ats[,4], k = 1))/ (lag(ats[,4], k = 1))

plot(daily_return)

```

5. Create a 20-day and a 50-day rolling mean series from the AMAZ.close series.

```{r}
head(cbind(ats[,4], rollapply(ats[, 4], 20, FUN = mean, na.rm = TRUE)),30)
head(cbind(ats[,4], rollapply(ats[, 4], 50, FUN = mean, na.rm = TRUE)),60)
```
