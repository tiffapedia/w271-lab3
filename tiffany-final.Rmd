---
title: "W271 Lab 3"
author: "Tiffany Jaya, Joanna Huang, Shan He, Robert Deng"
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

```{r}
# add packages
library(knitr)
# prevent source code from running off the page
opts_chunk$set(tidy.opts=list(width.cutoff=70), tidy=TRUE)
# remove all objects from current workspace
rm(list = ls())
# set seed number to reproduce results
set.seed(1)
# load data
# 1. ECOMPCTNSA: E-commerce retail sales as a percent of total sales
# https://fred.stlouisfed.org/series/ECOMPCTNSA
raw.sales <- read.csv('./data/ECOMPCTNSA.csv', header=TRUE, sep=',')
```

# Question 1: Forecasting using a SARIMA model

TODO: change the % and ARIMA model

Since the emergence of the internet, more and more people are shopping at online retailers than brick-and-mortar stores. E-commerce is on the rise, and we would like to see what percentage of total retail sales e-commerce is accounted for in the fourth quarter of 2017. With data from the US Census Bureau ranging from 1999 to 2016, we were able to estimate using the seasonal autoregresive integrated moving average model (or SARIMA for short) that e-commerce makes up approximately 10.44% of all retail sales. While the number does not seem substantial compare to the perceived value of e-commerce, we have to remember that retail sales include motor vehicles, gas stations, and grocery stores where e-commerce has yet to play a major role in the field. 

The SARIMA model that we use for the projected forecast is $\text{ARIMA}(0,0,0)(0,0,0)_4$. 

## Exploration Data Analysis 

The first step once we obtained the dataset was to examine its structure. 

```{r}
dim(raw.sales)
kable(summary(raw.sales)) 
sales <- ts(raw.sales$ECOMPCTNSA, start=c(1999,4), frequency=4)
head(sales); tail(sales)
```

We were able to determine that there was no missing value among the 69 observations with sales appearing to increase  overtime from 0.7% in the 4th quarter of 1999 to 9.5% in the 4th quarter of 2016. To confirm, we plot the time series as well as its associating MA(4) model. If the data expressed seasonality every quarter, the MA(4) model smooths out the variances and acts as an annual trend with the seasonal effects within each quarter removed.

```{r fig.show='hold', fig.align='center', out.width='49%'}
plot(sales, ylab = 'e-commerce sales (%)', main='E-commerce Quarterly Series')
lines(ma(sales, order=4, centre=T), col='blue')
acf(sales)
```

Given the upward trend and increasing variance, the series is non-stationary with quarterly seasonality. The autocorrelation function further substantiates the series's non-stationary because of its slow decay and multiple statistically significant values that are large and positive. For this reason, we will perform two operations. One, we will difference the series to stabilize the mean. And two, we will apply logarithmic transformation to stabilize the variance. We verified whether or not differencing is necessary using the unit root test.

```{r}
# unit root test
# H0: stationary, HA: p < 0.05, non-stationary
kpss.test(sales)
# number of differences required
nsdiffs(sales)
```

Small p-value in the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test confirms our intuition to difference the time series. Although the seasonal unit test suggests to difference the data once, we decided to difference it a second time in order to make it stationary.

```{r fig.show='hold', fig.align='center', out.width='49%'}
# first-order differenced series plot
plot(diff(sales, difference=1))
# first-order differenced log-transformed series plot
plot(diff(log(sales), difference=1))
# first-order differenced log-transformed series
tsdisplay(diff(log(sales), lag=4, difference=1))
# second-order differenced log-transformed series
tsdisplay(diff(log(sales), lag=4, difference=2))
# unit root test on first-order differenced log-transformed series
kpss.test(diff(log(sales), lag=4, difference=1))
# unit root test on first-order differenced log-transformed series
kpss.test(diff(log(sales), lag=4, difference=2))
```

## Modeling 

Looking at the autocorrelation function (ACF) and partial autocorrelation function (PACF) of the second order differenced, we found the best-fitting model to be $\text{ARIMA}(1,2,2)(1,2,1)$. We noticed that there are spikes in the PACF at lag 4 

(0,2,1)(2,2,1)
* Second order differenced log-transformed series: $\text{ARIMA}(1,2,2)(1,2,1)$
  * non-seasonal lags
    * 1 significant spike in PACF at lag 1 -> non-seasonal AR(1)
    * 2 significant spikes in ACF at lag 1,3 -> non-seasonal MA(2)
  * seasonal lags
    * 1 significant spike in PACF at lag 4 -> seasonal AR(1)
    * 1 significant spike in ACF at lag 4 -> seasonal MA(1)

By iterating through multiple parameters, we can confirm that this model is the best-fitting model under the AIC criterion.


```{r}
AIC(Arima(log(sales), order=c(1,2,3),seasonal=list(order=c(1,2,1),4)))
```


```{r}
base.m <- Arima(log(sales), order=c(1,2,3),seasonal=list(order=c(1,2,1),4))
best.m <- base.m
for(P in 0:2) {
  for(Q in 0:2) {
    for(p in 0:2) {
      for(q in 0:2) {
        m <- Arima(log(sales), order=c(P,2,Q), seasonal=list(order=c(p,2,q)))
        if(m$aic < best.m$aic) {
          best.m <- m
        }
      }
    }
  }
}
best.m
```

# Question 2: Learning how to use the xts library

If we could select one company to represent the e-commerce trend, Amazon is it. 